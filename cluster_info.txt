Connecting via SSH
Any user with a Palmetto Cluster account can log-in using SSH (Secure Shell) to access the terminal interface. Mac OS X and Linux systems come with an SSH client installed, while Windows users will need to download one.

SSH on Mac and Linux
Mac OS X or Linux users may open a Terminal, and type in the following command:

Palmetto 2
ssh username@slogin.palmetto.clemson.edu

where username is your Clemson user ID. You will be prompted for both your password and DUO authentication.

Cluster Architecture
Understanding the different hardware components of Palmetto and how they are connected or related is critical to making the best use of resources.

Overview
You may have heard the term "supercomputer" used to describe Palmetto. While not inaccurate, a better description would be that Palmetto is a high-performance computing (HPC) cluster.

The cluster is made up of over a thousand separate computers, called nodes. These nodes are connected through many networks, described as interconnects.

The diagram below shows a high-level view of the cluster.

Research Storage
Research Network
üå¥ Palmetto Cluster
Compute Nodes
Newer Phases
Older Phases
üîíSSH Connection
üîíSMB Connection
üîíSSH/SFTP Connection
Ethernet
Interconnect
Ethernet
Interconnect
Ethernet
Interconnect
Ethernet
Interconnect
Ethernet
Interconnect
Ethernet
Interconnect
Ethernet
Interconnect
InfiniBand
Interconnect
InfiniBand
Interconnect
InfiniBand
Interconnect
InfiniBand
Interconnect
üåÄ
Indigo Data Lake
üîÉ
Data Transfer Node
‚ÜîÔ∏è
Ethernet
Switch
‚ÜîÔ∏è
Infiniband
Switch
üîë
Login Node
‚è∞
Slurm Controller
(Scheduler Node)
üñ•
nodeA
üñ•
nodeB
üñ•
nodeC
üíª
User's Workstation
Compute Nodes
Most of the nodes in the cluster are compute nodes. These nodes have powerful hardware that can perform fast calculations on large amounts of data.

It is important to understand that Palmetto is a heterogeneous cluster, which means that compute nodes vary in hardware configuration across the cluster. This is different from many other HPC systems.

However, Palmetto is homogeneous within its phases, which means compute nodes in a given phase of the cluster have the same hardware configuration. You can see this on the Hardware Table.

Special Nodes
However, a cluster needs more than just compute nodes to be functional. Other special nodes in the cluster provide critical services to keep the cluster running or provide user access to resources.

Login Nodes
The login nodes are the interface between Palmetto and outside networks. Compute nodes are isolated from the internet, so you must connect to a login node first to gain access to them.

CAUTION
The login node is intended to be used only for scheduling jobs, connecting to compute nodes, and other light maintenance tasks. Performing intensive tasks on the login node may cause it to become unusable for other users.

You can access the login nodes via SSH connections.

Data Transfer Nodes
The data transfer nodes allow users to move large amounts of data in or out of Palmetto.

You can access the data transfer nodes via SSH file transfer protocol (SFTP) connections.

Scheduler Node
The scheduler node keeps track of what resources are available on the compute nodes and assigns those resources to people who request them.

You can request time on a compute node by submitting a job to the scheduler. The scheduler will add your job to the queue and determine which nodes are able to run your job and when they will be available.

Interconnects
Recall that the nodes in the cluster are connected via computer networks. These network connections, called interconnects are frequently classified by their type.

INFO
A single node can have multiple interconnects.

Each type of interconnect has a certain amount of bandwidth, which is measured in gigabits per second (Gbps). Depending on how much data you are moving between nodes, you will want to select an appropriate interconnect.

Ethernet
Ethernet is the slowest interconnect, but it is available on every node.

NOTE
Ethernet is the only interconnect available on the oldest nodes in the cluster.

Bandwidth varies depending on the model of network interface card installed in the node. However, the bandwidth will be the same for all nodes in a given phase.

One of the following will be available on every node:

1g - 1 Gbps ethernet
10ge - 10 Gbps ethernet
25ge - 25 Gbps ethernet
FDR Infiniband
The nodes on phases 7 through 17 have FDR Infiniband interconnects, which have a bandwidth of 56 Gbps. This is also referred to as 56g.

HDR Infiniband
The newest nodes on phases 18 through 29 have HDR Infiniband interconnects, which have a bandwidth of 100 Gbps. This is also referred to as 100g.

Cluster Type
However, Palmetto is homogeneous within its phases, which means compute nodes in a given phase of the cluster have the same hardware configuration.

TIP
If your job needs a certain type of hardware, make sure the resource list you pass to sbatch, salloc, or srun will target compatible nodes.

Hardware Table
close-up view of compute nodes

INFO
More information may be available if you log in!
Users with an active allocation on Palmetto 2 can view the number of idle/busy/offline nodes and see owner partitions they have access to.
General Partition: work1
This is a general access partition, which cumulatively contains the following resources: 436 nodes; 622 GPUs; 33,432 CPU cores; and 213,968 GB of memory.

Phase #	Node Count	Inter¬≠connect	CPU Make	CPU Gen¬≠eration	Core Count	Mem¬≠ory	GPU Make	GPU Model	GPU Count
ai01	1	hdr, 100g, 100ge	Intel	Haswell	40	500 GB	NVIDIA	V100	8
ai02	2	hdr, 100g, 100ge	AMD	Rome	128	1000 GB	NVIDIA	A100	8
ai03	7	hdr, 100g, 25ge	Intel	Sapphire Rapids	104	1000 GB	NVIDIA	H100	8
s00a	1	fdr, 56g, 10ge	Intel	Broadwell	40	1000 GB			
s00b	1	fdr, 56g, 10ge	Intel	Broadwell	40	1500 GB			
s00c	3	fdr, 56g, 10ge	Intel	Sky Lake	80	1500 GB			
s00d	4	fdr, 56g, 10ge	Intel	Cascade Lake	36	1500 GB			
s00e	1	hdr, 100g, 25ge	Intel	Ice Lake	56	1000 GB			
s00f	1	hdr, 100g, 25ge	AMD	Milan	64	1000 GB			
s00g	1	hdr, 200g, 25ge	AMD	Genoa	192	1500 GB			
s03c	4	fdr, 56g, 10ge	Intel	Sandy Bridge	32	750 GB			
s13a	2	hdr, 100g, 25ge	Intel	Sky Lake	40	370 GB	NVIDIA	V100	4
s13b	65	hdr, 100g, 25ge	Intel	Sky Lake	40	370 GB	NVIDIA	V100	2
s13c	10	hdr, 100g, 25ge	Intel	Sky Lake	40	750 GB	NVIDIA	V100	2
s14	28	hdr, 100g, 25ge	Intel	Cascade Lake	40	375 GB	NVIDIA	V100	2
s15	22	hdr, 100g, 25ge	Intel	Cascade Lake	56	370 GB	NVIDIA	V100S	2
s16	34	hdr, 100g, 25ge	Intel	Cascade Lake	56	370 GB	NVIDIA	A100	2
s17	26	hdr, 100g, 25ge	Intel	Ice Lake	64	250 GB	NVIDIA	A100	2
s18	40	hdr, 100g, 25ge	Intel	Ice Lake	64	250 GB	NVIDIA	A100	2
s19	65	hdr, 200g, 25ge	AMD	Genoa	192	750 GB			
s2019a	4	fdr, 56g, 10ge	Intel	Cascade Lake	48	370 GB			
s2022a	1	hdr, 100g, 25ge	AMD	Milan	64	500 GB			
s2022b	1	hdr, 100g, 25ge	Intel	Ice Lake	72	250 GB			
s2022c	1	hdr, 100g, 25ge	Intel	Cascade Lake	56	370 GB	NVIDIA	A40	2
s2023a	3	hdr, 100g, 25ge	Intel	Ice Lake	64	250 GB			
s2023b	1	hdr, 100g, 25ge	AMD	Milan	64	1000 GB	NVIDIA	A100	4
s2024a	1	hdr, 100g, 25ge	Intel	Sapphire Rapids	64	250 GB	NVIDIA	L40S	4
sky01a	22	fdr, 56g	Intel	Broadwell	20	125 GB			
sky01b	3	fdr, 56g	Intel	Broadwell	28	500 GB			
sky01c	6	fdr, 56g	Intel	Broadwell	20	60 GB	NVIDIA	?	4
sky01d	2	fdr, 56g	Intel	Broadwell	20	125 GB	NVIDIA	P100	1
sky02a	24	25ge	Intel	Cascade Lake	52	750 GB			
sky02b	5	25ge	Intel	Cascade Lake	52	1500 GB			
sky02c	6	25ge	Intel	Cascade Lake	52	370 GB	NVIDIA	RTX 6000	8
sky03	38	25ge	Intel	Sapphire Rapids	112	500 GB			


About Slurm
The RCD team is excited to welcome users to the new Slurm scheduling experience on Palmetto 2.

On the former Palmetto 1 cluster, the Portable Batch System (PBS) was used to schedule and manage jobs on the cluster.

How can I get started with Slurm?
Sign up for one of our training workshops to learn more about how to work with Slurm.

What is Slurm?
Slurm is a workload management tool that is designed to work with multiple Linux systems in a cluster environment. Its primary functions are is to job scheduling and resource allocation.

What makes Slurm different from PBS?
Under the hood, there are many differences between Slurm and PBS. However, most of these differences are transparent to users.

The primary difference that users will notice is different commands. For example, instead of using qsub, users must use srun, sbatch, or salloc.

Users are encouraged to review the Slurm Migration Help page for a direct comparison between the two systems and advice on how to convert existing workflows.

Why move to Slurm?
The way PBS handles scheduling often results in jobs getting stuck in the incorrect queue or never running at all. This was frustrating for users who were waiting for their jobs to run and a burden for our support staff to monitor and fix.

The switch to Slurm allows users to have more control over when, where, and how their jobs get scheduled. With its fair share algorithm, jobs will no longer get indefinitely stuck in queues due to request size or rare/limited resources.

Switching to Slurm also allows RCD to add more features to job scheduling, resulting in an overall better experience for Users on Palmetto 2. Since all the scheduling is done automatically by Slurm, instead of pseudo-manually with PBS, this also opens up time for RCD staff to better help users by developing helpful tools for Palmetto.
Slurm Migration Guide
Welcome! We are glad that you are interested in migrating your workflow to the new Palmetto 2 cluster using the Slurm job scheduler!

While there are many things that are different on the new cluster, there is a wealth of similarity between the two systems that will help existing users. This page has everything you need to get started.

New Account System
Accounts on the new Palmetto 2 cluster are separate from accounts on the Palmetto 1 cluster.

WARNING
Users will need to obtain an account on Palmetto 2 before proceeding with any of the other steps in this migration guide.

To learn more, see the account setup page.

New Login Node Address
The login node for Palmetto 2 has a different hostname, so you will need to use the new address when connecting via SSH.

Users should connect to the Palmetto 2 login node using this address: slogin.palmetto.clemson.edu.

TIP
The s at the front of the new address stands for Slurm! This should help you remember the new address.

If you connect to the right instance, you will see the updated Message of the Day (/etc/motd) with the words PALMETTO 2 at the top:

$ cat /etc/motd
 ------------------------------------------------------------------------------
             Welcome to the PALMETTO 2 CLUSTER at CLEMSON UNIVERSITY
                                    ...
 ------------------------------------------------------------------------------


WARNING
If you SSH to the old address, you will reach Palmetto 1 instead of Palmetto 2. Please double check the address before connecting.

You can review the updated SSH connection instructions for more details.

New OnDemand Instance
The new Palmetto 2 cluster has a separate instance of Open OnDemand. You will need to update your browser bookmarks.

The new address for Open OnDemand is: https://ondemand.rcd.clemson.edu

TIP
The new Open OnDemand for Palmetto 2 has a purple navigation bar at the top of the screen and the Palmetto 2 logo on the home page. This should help you ensure you are on the right instance.

screenshot of new ondemand instance showing purple navigation bar and updated logo

PBS to Slurm Command Map
The table below shows common PBS commands and their Slurm counterparts.

Description	PBS Command	Slurm Command
Submit a batch job	qsub (without -I)	sbatch ‚Äì see instructions for using sbatch
Submit an interactive job	qsub (with -I)	salloc ‚Äì see instructions for using salloc
Job statistics or information	qstat	multiple options ‚Äì see job monitoring options in Slurm
View job logs while running	qpeek	not needed ‚Äì see instructions for viewing job output
Cancel Job	qdel	scancel ‚Äì see instructions for using scancel
See available compute resources	whatsfree or freeres	whatsfree or sinfo ‚Äì see checking compute availability in Slurm
PBS to Slurm Environment Variables Map
The table below shows common PBS environment variables and their Slurm counterparts.

PBS Variable	Slurm Variable	Description
$PBS_JOBID	$SLURM_JOB_ID	Job id
$PBS_JOBNAME	$SLURM_JOB_NAME	Job name
$PBS_O_WORKDIR	$SLURM_SUBMIT_DIR	Submitting directory
cat $PBS_NODEFILE	$SLURM_JOB_NODELIST or srun hostname	Nodes allocated to the job
N/A	$SLURM_NTASKS	Total number of tasks or MPI processes (NOTE: not total number of cores if --cpus-per-task is not 1)
N/A	$SLURM_CPUS_PER_TASK	Number of CPU cores for each task or MPI process
Converting PBS Batch Scripts for Slurm
In PBS, users would use #PBS commands in their batch script files to inform the scheduler about what options they wanted to execute their job with. In Slurm, users must use #SBATCH comments instead.
In PBS, users use qsub job-script to submit the job to scheduler; in Slurm, users would use sbatch job-script instead.
NOTE: The modules loaded before the job is submitted will be carried to the batch job environment. Therefore, it is highly recommended to put module purge at the beginning of the job script.
For example, in PBS, users might use the job script like:

#!/bin/bash

#PBS -N my-job-name
#PBS -l select=2:ncpus=8:mpiprocs=2:mem=2gb:ngpus=1:gpu_model=a100:interconnect=hdr,walltime=02:00:00

export OMP_NUM_THREADS=8
python3 run-my-science-workflow.py


The same script, written for Slurm, would look like (we would recommend to use the long name for variables, for example using --nodes instead of -N):

#!/bin/bash

#SBATCH --job-name my-job-name
#SBATCH --nodes 2
#SBATCH --tasks-per-node 2
#SBATCH --cpus-per-task 8
#SBATCH --gpus-per-node a100:1
#SBATCH --mem 2gb
#SBATCH --time 02:00:00
#SBATCH --constraint interconnect_hdr

export OMP_NUM_THREADS=8
python3 run-my-science-workflow.py

Below are some brief explanations of parameters used here:

--nodes selects the number of nodes for the job, which is the same to the select using along with place=scatter in PBS, which means selecting different physical nodes not chunks.
--tasks-per-node is the number of tasks in each node, which is equivalent to the mpiprocs in PBS.
--cpus-per-task controls the number of cores in each task in the above. The default is 1 unless using multi-thread, where --cpus-per-task is usually set to the number for OMP_NUM_THREADS .
The total number of cores is not specified explicitly. It would be the value in --tasks-per-node multiplied by --cpus-per-task.
--mem is for memory per node.
--gpus-per-node can specify the gpu model and number of gpus per node following the format --gpus-per-node <gpu_model>:<gpu_number>.
--time is the walltime of the job, the max of which is 72 hours for c2 nodes.
interconnect on Slurm has not been fully implemented yet and not commented in the job script for now.
Converting PBS Interactive Job Workflows for Slurm
In PBS, users use qsub to request for interactive job; in Slurm, users will use salloc instead. (notice the command for interactive job (salloc) is different from the one for batch job sbatch)
NOTE: The modules loaded before the job is submitted will be carried to the interactive job environment. Therefore, it is highly recommended to run module purge once the interactive job is allocated.
For example, users can use the following command to request a PBS interactive job:

qsub -I -l select=2:ncpus=4:mem=2gb:ngpus=1:gpu_model=a100:interconnect=hdr,walltime=02:00:00


The corresponding command in Slurm would look like:

salloc --nodes 2 --tasks-per-node 4 --cpus-per-task 1 --mem 2gb --time 02:00:00 --gpus-per-node a100:1 --constrains interconnect_fdr


The explanations of the parameters can be found in the above section.

PBS Select Quantity vs Slurm Task Quantity
Although the syntax/usage of Slurm could be similar to PBS, there are some main differences. The most important one is --nodes is not required. Its value will be determined by the tasks requested:

If --tasks-per-node is specified, all the cpu cores will be allocated to the same node, which means the number of nodes is 1 in this case. NOTE: the number of tasks/cpu cores must be smaller than the number of cpu cores on a single node.
If you need more tasks/cpu cores than the number of cpu cores on a single node, but you don't care which nodes these cpu cores will be allocated, you can specify --ntasks. In this case, you job may wait less time in the queue since it can be landing on different nodes. A potential drawback is the performance of each cpu cores on different nodes might be different considering the heterogeneous nature of Palmetto cluster.
As mentioned above, --mem is for the memory per node. Besides --mem, there are some other options, such as memory per cpu, --mem-per-cpu and memory per gpu, --mem-per-gpu.
PBS Queues vs Slurm Partitions
In PBS, jobs were submitted to queues. In Slurm, the analogous concept is partitions.

To learn more, see the partition flag instructions.

Job Types
Understanding the correct job type to use is important when planning how to run your software on an HPC cluster.

There are two primary job types: Interactive and Batch, which are explained below.

Interactive Jobs
Starting an interactive job gives you a shell on a compute node. You can manually enter commands at the terminal to run them on the node.

Interactive jobs are great for:

short tasks where you can stay connected and wait for the results
writing, testing, and debugging your job scripts
TIP
Jobs submitted through Open OnDemand are considered interactive jobs.

DANGER
Interactive jobs will be terminated automatically if the system determines there is no user present to interact with the job.

This happens when:

your interactive session is left idle for too long
you disconnect from the interactive session (including loss of network connectivity).
Batch Jobs
Submitting a batch job will queue a shell script to run on a compute node. You must write the script ahead of time and plan the exact sequence of commands to be run on the node.

Your batch job will run in the background, so you can log out or disconnect from Palmetto while it is queued or running.

The scheduler will send you an email when your job is complete.

Batch jobs are great for when you:

have tasks that do not require interactive input
need a large amount of resources, since you may disconnect while the job waits in the queue to run
know your job will take a long time to execute, since you may disconnect while the job executes in the background
Comparison
The table below provides an at-a-glance comparison between the two different types and their use cases.

Metric	Interactive	Batch
Requires planning commands to run ahead of time.	‚ùå No	‚úÖ Yes
Can receive interactive keyboard input.	‚úÖ Yes	‚ùå No
Terminated when you disconnect from Palmetto.	‚úÖ Yes	‚ùå No
Runs in the background.	‚ùå No	‚úÖ Yes

Job Submission in Slurm
Now that you understand the basic types of jobs, you are ready to submit a job.

When you submit a new job, the scheduler places your job in the queue until it can find available resources to assign.

Slurm Job Submission Commands
Slurm has three different job submission commands: salloc, sbatch, and srun. Users will need to understand the difference between these commands to determine which command is best suited to the task they are trying to accomplish.

The salloc command
The salloc command is used to request resource allocation for interactive jobs.

salloc [JOB-SUBMISSION-FLAGS]

By default, without additional options, salloc will request one compute node, one CPU core, and one gigabyte of memory for thirty minutes. To customize your resource request, you can pass one or more job submission flags (such as --time or --mem) to specify what you need.

Once you submit your salloc job, it will follow this life cycle:

If the scheduler accepts your job, salloc will pause while your job waits in the queue for resources to be allocated.
When the job is allocated, salloc will provide an interactive shell session where you can type commands.
When your shell session exits, salloc will automatically mark your job as completed and release the resources for use by other jobs.
To learn more, you can read the salloc manual page.

Example usage of salloc

The sbatch command
The sbatch command is used to submit batch jobs that will run in the background.

sbatch [JOB-SUBMISSION-FLAGS] [SCRIPT-FILE]

Users must pass the path to a shell script (typically a .sh file) that will be invoked by sbatch when your job starts. The script must specify both the resource requirements for the job and what commands to run.

Your shell script should use special #SBATCH comments at the top of the file to specify resource requirements. You can use any of the standard job submission flags. If desired, you may override flags specified in the script file by passing them to sbatch before the script file name.

Below your #SBATCH comments, you should specify one or more commands that you would like to run during your task. We strongly recommend using srun to run commands in your batch script to make debugging easier.

Once you submit your sbatch job, it will follow this life cycle:

If the scheduler accepts your job, sbatch will output the Job ID for your newly created job.
After submission, you may disconnect from Palmetto and work on other things.
When the job is allocated, the shell script that you provided to sbatch will be executed.
When your shell script exits, the job will be marked as completed and resources will be released for use by other jobs.
Job output is saved to a file on disk by default. See the control page for instructions on how to view job output during or after execution.

To learn more, you can read the sbatch manual page.

Example usage of sbatch

The srun command
The srun command is typically used to launch programs as a parallel job within a batch job submitted by sbatch.

srun [JOB-SUBMISSION-FLAGS] [COMMAND-TO-RUN] [FLAGS-FOR-COMMAND-TO-RUN]

NOTE
srun may also be used outside of sbatch scripts to request other parallel job allocations, which is not covered on this page.

Using srun provides better integration with the Slurm scheduler and can help with debugging individual steps of a job. This is especially helpful when you are running more than one task (--ntasks greater than 1), possibly even across multiple nodes (--nodes greater than 1).

When used inside of an sbatch job script, commands passed to srun are run on all nodes/tasks in the job by default. You can alter this behavior using the --ntasks and --nodes flags.

If you would like to further subdivide the resources requested by your job, you can specify additional job submission flags.

To learn more, you can read the srun manual page.

Basic example usage of srun within an sbatch job with multiple nodes

Slurm Job Submission Flags
When submitting a job, you will need to use a combination of flags to specify which resources your job needs.

You can specify just the flags you need. For example, if you are not using a GPU, you do not need to specify --gpus-per-task 0.

RECOMMENDED FLAGS
At a minimum, we recommend that you specify wall time, CPU count, and memory.

Wall Time
Wall time is the maximum amount of time your job should take to complete. This amount is measured in real time (like a wall clock), which is different from CPU time.

The timer begins when your job starts executing, so the time your job spends waiting in the queue is not included in your wall time limit. Once the timer expires, your job will be terminated if it is still running.

Use flag --time (or lowercase -t for short), and format your request as hh:mm:ss.

For example, to ask for 80 hours, 39 minutes, and 20 seconds:

--time 80:39:20

WALL TIME LIMITS
To help us better allocate the limited pool of shared resources, there is a maximum amount of wall time that users may request for a job. The limit varies based on job source and the destination partition you submit to.

Job Source	General Partitions	Owner Partitions
Batch Job Submission (via sbatch)	72 hours (3 days)	336 hours (14 days)
Interactive Job Submission (via salloc)	12 hours	336 hours (14 days)
OnDemand Interactive App Session	12 hours	12 hours
CPU Core Count
By default, jobs will only receive one CPU core per task.

Use flag --cpus-per-task (or lowercase -c for short).

For example, to ask for 7 CPU cores:

--cpus-per-task 7

Memory
There are many ways to specify the amount of memory your job requires in Slurm.

The best route will depend on how many tasks your job requests.

Single-Task Jobs
Multi-Task Jobs
This recommendation applies only to single-task jobs (where ntasks is not specified or ntasks=1).

Use flag --mem and provide the amount of memory you need.

For example, to ask for 2.5 gigabytes:

--mem 2.5gb

CAUTION
The default unit is megabytes (mb), so be sure to specify gb if you intend to ask for gigabytes of memory.

Job Name
Users can give their jobs a unique name to make it easier to find them in reports.

Use flag --job-name (or uppercase -J for short) and provide the desired name. Note that these names should not include spaces.

For example, to use name dice_roll_simulation:

--job-name dice_roll_simulation

Email Notifications
Users can request email notifications on events. The flag to enable this is:

--mail-type <type>

Type can be one or more of the following:

NONE: no emails (default)
BEGIN: job began
END: job ended
FAIL: job failed
REQUEUE: job dequeued
ALL: the same as BEGIN,END,FAIL,INVALID_DEPEND,REQUEUE,STAGE_OUT
INVALID_DEPEND: dependency never satisfied
TIME_LIMIT: reached time limit
TIME_LIMIT_90: reached 90% of time limit
TIME_LIMIT_80: reached 80% of time limit
TIME_LIMIT_50: reached 50% of time limit
ARRAY_TASKS: send email for each array task
GPU Count
By default, no GPU resources are assigned to jobs. Users can ask for GPU resources by specifying the number of GPUs they need and (optionally) the desired model of GPU.

Use flag --gpus or lowercase -G for short, and pass either:

the number of GPUs you want, of any type
the type of GPU you want and the number of GPUs you want, separated by a colon (:)
For example, if you want two GPUs of any type:

--gpus 2

Or, if you want a single NVIDIA A100 GPU:

--gpus a100:1

What GPU models are available?
What if I need a specific VRAM variant of a particular GPU model?
What if I am running multiple tasks?
Partition Target
Users can specify which partition they would like to submit their job to. Partitions are analogous to queues.

If you have access to purchased nodes on the cluster, submitting to the associated owner partition will grant your job priority over general partition jobs.

Use flag --partition (or lowercase -p for short) and pass the name of the partition.

How can I find out what partitions I have access to?

Output File Location
Feature Constraints
Advanced users of Palmetto may need to request a very specific type of hardware. Feature constraints allow you to narrow down what resources your job is eligible to run on.

Use flag --constraint or uppercase -C for short, then pass a comma-delimited list of features that you require.

For example, say you need a node with interconnect type FDR and an Intel CPU:

--constraint interconnect_fdr,chip_manufacturer_intel

CAUTION
While many options in Slurm are common across compute clusters, there is no standard convention for feature values. If you are copying code from elsewhere that uses feature constraints, make sure you are using the correct feature values from below for Palmetto.

Interconnects
To specify which interconnect you want, the following features are available:

Interconnect Type	Feature Value
1 Gigabit Ethernet	interconnect_1ge
10 Gigabit Ethernet	interconnect_10ge
25 Gigabit Ethernet	interconnect_25ge
100 Gigabit Ethernet	interconnect_100ge
HDR InfiniBand (100 Gbps)	interconnect_hdr or interconnect_100g
FDR InfiniBand (56 Gbps)	interconnect_fdr or interconnect_56g
CPU Manufacturer
To specify which CPU manufacturer you want, the following features are available:

CPU Manufacturer	Feature Value
Intel	chip_manufacturer_intel
AMD	chip_manufacturer_amd
CPU Generation
CPU Generation	Feature Value
AMD Genoa	cpu_gen_genoa
AMD Milan	cpu_gen_milan
AMD Rome	cpu_gen_rome
Intel Broadwell	cpu_gen_broadwell
Intel Cascade Lake	cpu_gen_cascadelake
Intel Haswell	cpu_gen_haswell
Intel Ice Lake	cpu_gen_icelake
Intel Sapphire Rapids	cpu_gen_sapphirerapids
Intel Sky Lake	cpu_gen_skylake
GPU VRAM Variant
To specify which VRAM variant of a particular GPU you want, the following features are available:

GPU Model	VRAM Amount	Feature Value
NVIDIA A100	40 GB	gpu_a100_40gb
NVIDIA A100	80 GB	gpu_a100_80gb
INFO
Specifying the feature flag does not request a GPU. For example, if you specify --constraint gpu_a100_80gb, but do not also specify --gpu a100:1, then you would not receive a GPU.

The feature constraint simply filters out which VRAM variant of a particular GPU your job is eligible to run on.

GPU Interconnects
GPU Interconnect Type	Feature Value
NVLink	gpu_interconnect_nvlink
PCI Express (PCIe)	gpu_interconnect_pcie
INFO
If you do not specify a constraint, the default is PCI Express (PCIe).

Task and Node Count
For advanced users, you may need access to multiple chunks of compute resources to complete your job. In particular, this is useful for MPI.

Use flag --ntasks or lowercase -n for short. Pass the number of tasks you want to run as an argument.

For example, if I want to run four tasks (MPI processes):

--ntasks 4

INFO
Multiple tasks can land on the same or different compute nodes depending on the available resources.

To specify how many nodes you want, you can use flag--nodes along with --ntasks.

For example, to run four tasks across two different compute nodes:

--nodes 2 --ntasks 4

INFO
It is possible the four tasks (MPI processes) are not allocated evenly on the two nodes.

To allocate the four tasks (MPI processes) evenly on the two nodes, you can use flag --nodes together with --tasks-per-node.

For example, to run four tasks across two different compute nodes, with two tasks on each node:

--nodes 2 --tasks-per-node 2

For the same job with the four tasks (MPI processes) allocated evenly on the two nodes, if you want to allocate 6 threads for each task, you can use flag --cpus-per-task.

--nodes 2 --tasks-per-node 2 --cpus-per-task 6

This usually works together with the OMP_NUM_THREADS environment variable, which should be set to the same number of --cpus-per-task by:

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

The $SLURM_CPUS_PER_TASK is a Slurm environment variable. More Slurm environment variables can be found in the next section.

OpenMPI
On Slurm cluster, the prebuilt OpenMPI (both version 4.1.5 and 4.1.6) were compiled with pmi2 support enabled. Therefore, if your application is compiled using the prebuilt OpenMPI, your applications can be launched directly using the srun command. The resources, such as the number of nodes, number of MPI processes, number of threads for each MPI process, memories, etc will be determined automatically according to your input for parameters --nodes, --ntasks-per-node, --cpus-per-task, --mem, etc.

Intel MPI
On Slurm cluster, it is also recommended to use the srun command if the application is built against Intel MPI. This method is the best integrated with Slurm and supports process tracking, accounting, task affinity, suspend/resume and other features. However, Intel MPI does not link directly to any external PMI implementations, so the users need to point manually PMI2 library. This can be done by the command:

export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi2.so

Then the user can launch the application by the srun command. The resources, such as the number of nodes, number of MPI processes, number of threads for each MPI process, memories, etc will be determined automatically according to your input for parameters --nodes, --ntasks-per-node, --cpus-per-task, --mem, etc.

srun ./your_app_name

Available Environment Variables in a Slurm Job
The table below shows common Slurm environment variables. For a full list the Slurm environment variables, please visit the official document.

Variable Name	Description
$SLURM_JOB_ID	Job id
$SLURM_JOB_NAME	Job name
$SLURM_SUBMIT_DIR	Submitting directory
$SLURM_JOB_NODELIST or srun hostname	Nodes allocated to the job
$SLURM_NTASKS	Total number of tasks or MPI processes (NOTE: not total number of cores if --cpus-per-task is not 1)
$SLURM_CPUS_PER_TASK	Number of CPU cores for each task or MPI process
Job Control with Slurm
The new Palmetto 2 cluster uses the Slurm Workload Manager to schedule jobs.

Lifecycle of a Slurm Job
The life of a job begins when you submit the job to the scheduler. If accepted, it will enter the Pending state.

Thereafter, the job may move to other states, as defined below:

Pending - the job has been accepted by the scheduler and is eligible for execution; waiting for resources.
Held - the job is not eligible for execution because it was held by user request, administrative action, or job dependency.
Running - the job is currently executing on the compute node(s).
Completed - the job finished executing successfully.
Failed - the job finished executing, but returned an error.
Timeout - the job was executing, but was terminated because it exceeded the amount of wall time requested.
Out of Memory - the job was executing, but was terminated because it exceeded the amount of memory requested.
Canceled - the job was canceled by the requestor or a system administrator.
This diagram below provides a visual representation of these states:

Final

Waiting

Submit
Start Job
Preempted
Completed
Failed
Timeout
Out of Memory
Canceled
Pending
Held
Start
End
Running
For more details, you can review the Job State Codes in the Slurm Documentation.

Slurm Job Control Commands
This section provides essential commands and approaches to efficiently monitor and manage jobs within the Slurm workload manager on Palmetto.

Listing Slurm jobs with squeue
You can list the status of your current jobs using the squeue command:

squeue -u <username>

where you replace <username> with your Palmetto user name. This returns basic information for your queued and running jobs. You can control the output fields and format using the --format flag. For instance,

squeue -u <username> --format "%all"

will show all information available for each job.

For more information, see the official Slurm squeue documentation. Find additional examples at the bottom of the page. You can also access this documentation by running man squeue from a terminal.

Checking on Slurm jobs with scontrol
You can list detailed information about a specific job using the scontrol command:

scontrol show job <job-id>

where you replace <job-id> with the ID of a running or queued job. Retrieve this ID using the squeue command as described above. The scontrol show job command gives detailed information about your job including information about the resources allocated to the job.

In this example, we used scontrol to show job information. However, this powerful command also interacts with other aspects of the slurm configuration including nodes and partitions. For instance, you can view detailed node information with scontrol show node <hostname> where you replace <hostname> with the host name for a specific node.

For more information, see the official Slurm scontrol documentation. You can also access this documentation by running man scontrol from a terminal.

See the output of a Slurm job during execution
Slurm logs your job outputs to a text file. By default, this log file is located in the working directory used when creating the job and has the naming format slurm-<job-id>.out, where <job-id> matches the job ID returned by squeue. The name and location of the log file can be changed when submitting a job. You can view the output of your running job using standard linux commands. For instance, you can display live updates using the tail command:

tail -f path/to/slurm-<job-id>.out

The -f (follow) flag gives continuous updates as the file changes.

Canceling a Slurm job with scancel
You can terminate a running Slurm job and free the associated resources using the scancel command:

scancel <job-id>

where <job-id> matches the job ID returned by squeue.

TIP
To cancel all jobs submitted by your user, you can use the following command:

scancel -u $USER

For more information, see the official Slurm scancel documentation. You can also access this documentation by running man scancel from a terminal.

Connecting to a running Slurm job
Sometimes you may wish to connect to a running job for performance monitoring or debugging purposes. For instructions, refer to our job monitoring documentation.

Viewing available Slurm partitions to run your job
The sinfo -s command allows you to see all partitions you have access to. For example:

sinfo -s

Example output could be:

PARTITION AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST
work1*       up 3-00:00:00        3/16/0/19 node[0401,0403-0419,0421]
rcde         up   infinite        3/18/0/21 node[0400-0401,0403-0419,0421,1036]
rcdeprio     up   infinite          1/3/0/4 node[0400-0401,0404,1036]
training     up    2:00:00          0/1/0/1 node0400


This would mean I have access to the work1 (The * indicates this is the default), rcde, rcdeprio, and training partitions. To run a job with a particular partition, use -p <partition-name> on srun, sbatch, or salloc.

Check Availability of Slurm Nodes
There are several methods to figure out what hardware is available, busy, and offline in the cluster.

Using the online hardware table
You can visit the Hardware Table page on the documentation website to get a live view of the cluster's hardware availability in your browser.

TIP
The in-browser hardware table automatically updates once per minute if you are logged in!

Using sinfo
The Slurm standard sinfo can be used to check current cluster status and node availability.

Partition Summary
To generate a row per partition with summary availability information, use the following command.

sinfo -s

Example output:

PARTITION AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST
work1        up   infinite        3/18/0/21 node[0400-0401,0403-0419,0421,1036]


The NODES(A/I/O/T) column lists the nodes Allocated (in-use), Idle, Other, and Total.

Per Node Configuration Summary
To generate a row per each node configuration (CPU, memory, and GPU count) with summary availability information, use the following command.

sinfo -eO "CPUs:8,Memory:9,Gres:14,NodeAIOT:16,NodeList:50"

Example output:

CPUS    MEMORY   GRES          NODES(A/I/O/T)  NODELIST
64      256831   gpu:a100:2    4/16/0/20       node[0400-0401,0403-0419,0421]
128     1031142  gpu:mi210:1   0/1/0/1         node1036


The CPU column provides the total core count, the Memory column provides the number of megabytes of memory, the GRES column provides any GPUs, and the NODES(A/I/O/T) column lists the nodes Allocated (in-use), Idle, Other, and Total.

Checking Nodes Partially In Use
Although a node may be listed as allocated, there may be some resources left. This puts the node in a "mixed" state. We can check nodes in this state and see how much CPU and GPUs are available with the following command.

sinfo -NO "CPUs:8,CPUsState:16,Memory:9,AllocMem:10,Gres:14,GresUsed:24,NodeList:50" -t mixed


Example output:

CPUS    CPUS(A/I/O/T)   MEMORY   ALLOCMEM  GRES          GRES_USED               NODELIST
64      20/44/0/64      256831   4096      gpu:a100:2    gpu:a100:0(IDX:N/A)     node0403
64      45/19/0/64      256831   4096      gpu:a100:2    gpu:a100:2(IDX:0-1)     node0404
64      61/3/0/64       256831   8192      gpu:a100:2    gpu:a100:1(IDX:0)       node0405


This output provided the number of number of cores by state in the format "allocated/idle/other/total" in the column CPUS(A/I/O/T). It also shows the memory already in use (ALLOCMEM) and GPUs in use (GRES_USED).

In this example, the first node has 44 cores, 252GB memory (256GB - 4GB), and 2 GPUs free. The second has 19 cores, 252GB memory, and no GPUs free. The third has 3 cores, 248GB memory, and 1 GPU free.

This command only lists nodes in a mixed state, it will not show jobs that are completely idle or busy.

Estimating Job Start Time
When a job is queued, you can see its estimated start time by passing the --start flag to squeue. For example:

[dndawso@slogin001 ~]$ squeue --start --job 3103
             JOBID PARTITION     NAME     USER ST          START_TIME  NODES SCHEDNODES           NODELIST(REASON)
              3103     work1 interact  dndawso PD 2023-12-05T23:07:12     19 (null)               (Resources)


Here I see that job ID 3103 is currently pending (state is PD) and the reason is waiting for resources. The scheduler predicts that the job should start December 5th at 11:07:12pm. This is subject to change if higher priory jobs are submitted.

You can also estimate how long it would take before a job starts without submitting by passing --test-only to srun or sbatch. For example:

[dndawso@slogin001 jobperf-batch-test]$ sbatch --test-only run2.sh
sbatch: Job 3104 to start at 2023-12-05T23:07:12 using 19 processors on nodes node[0401,0403-0419,0421] in partition work1


In this example, the job was not actually submitted and queued, but I was told that the scheduler predicts that the job should start December 5th at 11:07:12pm. This is subject to change if higher priory jobs are submitted.

Overview
Today's scientific research involves increasingly large volumes of increasingly complex data. At the same time, CPU performance has improved well beyond storage media, which can create bottlenecks in HPC environments. Read this section to learn how to optimize your storage usage and improve I/O performance on Palmetto.

Palmetto provides three data spaces: home, scratch, and group project storage.

Every user has a home directory and access to the scratch file systems. Group project storage is allocated for faculty to house their data and can only be accessed by users having the owner‚Äôs approval. Each data space is accessible from anywhere in the cluster.

Storage Hardware Grid
The table below describes each file system on Palmetto and the hardware for the storage medium and connection to the nodes.

Name	Size	Disk Type	File System	Connection Type	Backup
/home	250GB per user	SSD	Indigo	Network (IB, ethernet)	‚úÖ yes
/scratch	5 TB per user	SSD	Indigo	Network (IB, ethernet)	‚ùå no
/local_scratch	99GB - 2.7TB per node	HDD, SSD, NVMe	ext4	Internal (SAS, SATA, PCIe)	‚ùå no
/project/{path}	Varies by owner	SSD	Indigo	Network (IB, ethernet)	‚úÖ yes
What do the abbreviations in the storage hardware grid mean?
Performance Guidelines
Generally speaking, /local_scratch will always be the fastest file system to use because there is no network involved. However, this space cannot be shared between a group of nodes participating in a job, and the data must be moved to permanent storage before the job completes.

/scratch is our newest parallel file system that runs atop flash storage. It performs well across most access patterns.

The RCDE team encourages you to test your workflows against all three file systems to see which one works best for you.

Home Directories
A user‚Äôs home data space is accessed by /home/{username} from the command line. Permissions are set to only allow that {username} to access their own directory. Each home directory:

has a quota of 250GB
is backed up daily
has hardware failure protection
is backed by our Indigo file system.
The user‚Äôs home data space is NOT to be used to process data or as a job‚Äôs working directory but is used to store permanent files.

What kinds of files should be stored in my home directory?
WATCH YOUR QUOTA!
You cannot log in to the system if your home directory is full (over quota). If this happens to you, please submit a support ticket and a system administrator will help you fix the overage.

Check Home Directory Quota
You can check the quota and usage for your home directory using the checkquota command on the login node.

user@login001 $ checkquota

HOMEDIR QUOTA for USER example

Max  quota = 250G
Used quota = 1.44G

(used quota is refreshed every 2 minutes)

Cleaning up your home directory
To investigate which files and folders are using large amounts of storage, you can use the du command. For example, to check the sizes of all files and folders in your home directory (non-recursively), run:

srun du -h -d 1 $HOME

If you find a folder you want to investigate (say /home/<user>/.local), you can run du on that directory too:

srun du -h -d 1 $HOME/.local

You can perform this recursively, moving down the directory tree looking for where all your space is being used. For example, if within your .local folder you found most of the space was in share, you could see where in share space is being used with:

srun du -h -d 1 $HOME/.local/share

Once you find where all your space is being used, it may become clear what is causing the issue. We have a few common cases listed below. If you still can't figure out what needs to be cleaned up or how, please reach out to us by submitting a support ticket.

Trashed files (~/.local/share/Trash)
When you "delete" files through some graphical software programs, like Jupyter Notebooks, the data is not actually removed from disk. Instead, it is moved to a Trash folder.

You can check the size of the Trash folder with:

srun du -sh ~/.local/share/Trash

To empty the trash and permanently delete all files within in it, run:

srun rm -rf ~/.local/share/Trash/*

Pip Cache (~/.cache/pip)
If you install pip packages in the default Anaconda environment, pip will use the ~/.cache/pip directory to cache downloads and wheels.

To check how much cache pip is using, run:

srun sh -c 'module load anaconda3; pip cache info'

To clear the cache, run:

srun sh -c 'module load anaconda3; pip cache info'

Anaconda (~/.conda)
You may notice that your .conda folder is large. This folder stores your conda environments. Anaconda comes with a handy command to clean up unneeded packages and caches. You can use the command below to run the cleanup process on a compute node:

srun sh -c 'module load anaconda3; conda clean --all'

If this doesn't clean up enough, you may want to delete any unneeded environments. First, you may want to see which environments are taking up the most space. Unfortunately, this is not trivial since data for environments are split between two locations. Packages installed with pip will be placed in ~/.conda/envs/<env-name>, where as conda packages will be placed in ~/.conda/pkgs and shared between any conda environment that needs it.

So the following command will display the size of the pip packages installed, but not necessarily the full size of the environment -- some data will be stored in ~/.conda/pkgs:

srun du -sh ~/.conda/envs/*

Once you decide you want to delete an environment, you can use conda remove to delete the environment and conda clean to clean up any packages that became unused:

srun sh -c 'module load anaconda3; conda remove --name <env-name> --all'
srun sh -c 'module load anaconda3; conda clean --all'


Temporary Scratch Space
Palmetto provides a space to store intermediate scratch data that is accessible on every node.

The two current scratch file systems are: /scratch, and /local_scratch.

AVOID DATA LOSS
Take care not to leave precious research results or programming assignments in scratch space for very long. Move files you need to keep to a permanent storage medium.

DANGER
Scratch spaces are never backed up. Files deleted or automatically purged from scratch space are permanently destroyed. Our support team cannot recover files stored in scratch space.

Shared Scratch (/scratch)
A large amount of scratch space is available to all users on the cluster through the shared scratch space.

We have designed the shared scratch file system to be resilient against hardware failure. If a disk fails, in most cases the system can continue operating without data loss. However, understand that no backups are available.

Shared Scratch Limits
Every Palmetto user has their own directory in /scratch under their username (/{filesystem}/{username}). There are larger limits (compared to your home directory) to the number and size of the files you can store in your scratch space.

Per-User Storage Limit	Per-User File Count Limit
5 TB	5 million files
Need a higher limit on /scratch for your account?
How do I check how many files I have in shared scratch?
How do I check my disk space usage on shared scratch?
Shared Scratch Purge Schedule
To ensure that space remains available for everyone, our system will periodically purge old files. Any files that have not been modified within a defined number of days will be automatically deleted.

Purge Criteria	Purge Schedule
30 days since last modified timestamp	11:00 PM EDT daily
CAUTION
Accessing or reading a file will not prevent it from being purged. The last modified timestamp determines purge eligibility.

How can I check the date a file was last modified?
Can I get a list of files that are at risk of being purged?
Can I just touch or copy my files to avoid the purge?
Local Scratch (/local_scratch)
Every Palmetto compute node has storage on the node itself and is referred to as /local_scratch. This scratch space can only be accessed by a job running on the node. The file system for /local_scratch has no hardware failure protection and is never backed up.

To access this space in your PBS script, use the $TMPDIR environment variable or the location /local_scratch/pbs.$PBS_JOBID. If you are using multiple nodes, further setup is required.

CAUTION
Files in /local_scratch will be purged automatically when your job terminates. Copy files elsewhere before your job ends.

You can see the amount of storage by looking at the hardware table from the login node:

user@login $ cat /etc/hardware-table

The hardware table identifies compute nodes by phases. Each phase has a specific disk medium and size to house its /local_scratch.

NOTE
Local scratch space is shared between all jobs running on a node at the same time.

How do I set up local scratch for use on multiple nodes?

Project Storage (/project)
Project storage on Palmetto uses the Indigo Data Lake. Faculty members can request storage allocations using the PDF forms provided on storage purchasing guide.

DANGER
Project storage is not currently managed via ColdFront.

More details about how to use Indigo can be found on the Indigo Data Lake documentation. Information about using Indigo on Palmetto is covered by the Indigo HPC Access section.

Requesting
Faculty interested in requesting project storage should consult the storage purchasing guide.

Check Quota
To check how much of your storage quota you are using on your Indigo project space from the Palmetto cluster, you can use the checkproject command.

The command takes one argument that indicates which space you want to check. Place dashes between the owner's username and the name of the space.

WARNING
The checkproject command is only available on the login nodes.

For example, say I have project space at /project/tiger/football, I could run the command:

$ checkproject tiger-football

Date: Mon Jan  1 12:30:44 EDT 2024
=====================================
Project Space   : /project/tiger/football
Total Capacity  : 3096 GB
Used            : 1104 GB
Available       : 1992 GB

You can use the Total Capacity, Used, and Available values displayed in the output to understand your quota and usage.

TIP
There are other methods for checking your quota! Visit the quota page on the Indigo Data Lake documentation to learn more.

Alternative Access Methods
Project storage make use of our Indigo Data Lake. Thus, owners can request certain alternative access methods be enabled for their project space (for example SMB mounts). For details, see the Indigo Data Lake access methods documentation.

Storage Technologies
Palmetto leverages high performance storage media and file systems to complement its high performance compute hardware. When used correctly, these technologies can accelerate your jobs and decrease the number of idle CPU cycles waiting for data.

Storage Media
Palmetto has two types of storage media, hard disk drives and flash drives, each having their own performance characteristics. The hard disk drives have rotating platters and a moving read/write head, the physical mechanisms by which a sector of data is accessed. In contrast, the flash drives require no moving parts to access data, just electrical signals, and so, the flash drives far outperform the hard drives simply due to the nature of the physical media.

Both hard drives and flash drives have their place on Palmetto. Hard drives cost much less per terabyte than flash drives and are very reliable. So, we use hard drives for long term storage to house pools of data, while flash drives are used for computing that require short lived, high I/O throughput.

File Systems
Contributing to the performance of any storage media is the file system that manages the data. Palmetto engages two types of file systems, traditional and parallel. Both types of file systems take advantage of the underlying storage medium; however, these file systems access data in divergent ways to provide the best possible performance.

Traditional file systems, such as ext4, xfs, and zfs, perform best when requests originate from the same machine as the location of storage. Palmetto compute nodes provide a limited amount of this kind of storage, referred to as local storage, that can only be used by an active PBS job. Palmetto also uses a software package called NFS, Network File System, which allows compute nodes to access data housed in a traditional file system on a storage server over a network.

Parallel file systems, such as BeeGFS, Lustre, and OrangeFS, perform best when there are many storage servers and a fast network between the compute nodes and the storage servers. These file systems spread data across a set of storage servers, optimizing performance by taking advantage of the processing power of each server and the fast network, while amortizing the time to process a request across the number of storage servers. That is, if a data request takes X amount of time using a traditional file system over a network, then the same request will take X/4 amount of time in a 4 storage server parallel file system. These file systems do not use NFS to communicate with the storage servers but have a client process on each compute node.

Public Datasets
RCD hosts a number of widely used publicly available datasets on Indigo, available at /datasets. This eliminates the need for users to consume space in their Home, Project, or Scratch spaces.

Please submit a ticket to request changes/additions to this list.

Dataset	Path	Description
AlphaFold	/datasets/alphafold	Genetic databases for running AlphaFold inferences. Use --data_dir=/datasets/alphafold when running your model.
BLAST	/datasets/blast/db	nt,nr,refseq_protein,refseq_rna,swissprot databases for use with NCBI BLAST+. Put export BLASTDB=/datasets/blast/db at the beginning of your batch scripts or in ~/.bashrc to use these databases.
iGenomes	/datasets/igenomes	Reference genome sequences download from AWS iGenomes. For use in nf-core Nextflow pipelines, see the usage instructions
ImageNet	/datasets/imagenet	


Overview
There are many ways to move data in and out of Palmetto for your research. Selecting the correct method is important, since using the wrong method might impact the performance of shared resources for other users.

Which method to use?
Globus and SFTP via the data transfer nodes (DTNs) are the recommended methods to transfer data between filesystems, as well as into and out of the cluster. The DTNs contain the fastest connections to all storage systems and are dedicated resources for transferring data.

Below are the supported methods to transfer data into Palmetto.

DANGER
Excessive SFTP data transfer activity on the login and Open OnDemand resources may make these shared systems slow or unusable for other users. If you exceed the limits below, system administrators may terminate your data transfer without warning to protect these shared resources.

Method	Maximum Transfer Size
Globus	All Transfers
SFTP (Data Transfer Nodes)	All Transfers
SFTP (login nodes)	Small files (< 100 MB total)
Open OnDemand	Very small files (< 50 MB total)
rclone	All Transfers
Globus
For larger files, we recommend using the Globus file transfer application. Globus provides a secure, unified interface for research data and gracefully handles transfers of terabytes of data.

In addition to transfers from your personal computer, Globus can also:

transfer data to and from Box
securely share your data on Indigo with external users
SFTP
Transfers can be completed via SFTP through your SSH client by connecting to one of Palmetto's dedicated data transfer nodes. These nodes are set up to ensure data transfer activity does not impact other users.

Open OnDemand
Open OnDemand provides a web interface for browsing file systems on Palmetto. You can upload and download files through your web browser without installing any extra software.

To keep the OpenOD web server performant, the maximum upload size is restricted to 50 MB total.

For more information, see the OpenOD Files Dashboard page.

Rclone
Rclone is a command line tool for transferring files to and from cloud storage products (over 70 cloud storage providers are supported). We have several examples of using Rclone from the Data Transfer Node to move files from Google Drive, Microsoft OneDrive, and Box.