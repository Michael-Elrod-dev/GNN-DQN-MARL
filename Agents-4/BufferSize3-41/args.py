import os
import time


class Args:
    def __init__(self):
        # Env Parameters
        self.env              = None
        self.num_agents       = 4
        self.num_goals        = 20
        self.num_obstacles    = 5
        self.grid_size        = 41
        self.agent_view_size  = 9
        self.action_size      = 4
        self.obs_size         = ((1 + self.num_goals) * 2) + self.num_goals

        # DQN Parameters
        self.prio_e           = 0.1
        self.prio_a           = 0.5
        self.prio_b           = 0.4
        self.double_dqn       = True
        self.priority_replay  = True

        # Training parameters
        self.total_steps      = 2500000
        self.episode_steps    = 150
        self.eps_start        = 1.0
        self.eps_end          = 0.01
        self.eps_percentage   = 0.80
        self.seed             = int(time.time())

        # Network Parameters
        self.buffer_size  = 50000    # Replay buffer size
        self.batch_size   = 64       # Sample batch size
        self.gamma        = 0.99     # Discount factor
        self.tau          = 0.001    # Soft update of target parameters
        self.lr           = 0.0005   # Learning rate 
        self.update_step  = 4        # Update the network

        # GNN Parameters
        self.gnn_hidden_size      = 16
        self.gnn_num_heads        = 3
        self.gnn_concat_heads     = False
        self.node_obs_shape       = 8
        self.edge_dim             = 1
        self.num_embeddings       = 3
        self.embedding_size       = 12
        self.gnn_layer_N          = 2
        self.gnn_use_ReLU         = True
        self.graph_aggr           = 'node'
        self.global_aggr_type     = 'mean'
        self.embed_hidden_size    = 16
        self.embed_layer_N        = 1
        self.use_orthogonal       = True
        self.embed_use_ReLU       = True
        self.use_feat_norm        = True
        self.embed_add_self_loop  = False
        self.full_features        = True
        self.max_edge_dist        = 4.5

        # Run Parameters
        self.device       = 'cuda'
        self.load_policy  = False    # Evaluate a learned policy
        self.logger       = True     # Log training data to Wandb
        self.render       = False     # Render the environment
        self.debug        = False     # Allow manual action inputs

        # Reward Parameters
        self.reward_goal = 10
        self.penalty_goal = -1
        self.penalty_obstacle = -5
        self.penalty_invalid_move = -5

        # Derived Parameters
        self.title = os.path.basename(os.path.dirname(os.path.abspath(__file__)))
        self.eps_decay = self.calc_eps_decay(self.eps_start, self.eps_end, self.total_steps, self.eps_percentage)

       
    # Calculate the rate that Îµ should decay
    def calc_eps_decay(self, eps_start, eps_end, n_steps, eps_percentage):
        effective_steps = n_steps * eps_percentage
        decrement_per_step = (eps_start - eps_end) / effective_steps
        return decrement_per_step